{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# forecasting_demand\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Startup cells"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Set environment variables for sagemaker_studio imports\n\nimport os\nos.environ['DataZoneProjectId'] = 'cgk5ugdebva2ef'\nos.environ['DataZoneDomainId'] = 'dzd-bsa8pqfbpcm7on'\nos.environ['DataZoneEnvironmentId'] = 'cbebxulwrbe6g7'\nos.environ['DataZoneDomainRegion'] = 'us-east-1'\n\n# create both a function and variable for metadata access\n_resource_metadata = None\n\ndef _get_resource_metadata():\n    global _resource_metadata\n    if _resource_metadata is None:\n        _resource_metadata = {\n            \"AdditionalMetadata\": {\n                \"DataZoneProjectId\": \"cgk5ugdebva2ef\",\n                \"DataZoneDomainId\": \"dzd-bsa8pqfbpcm7on\",\n                \"DataZoneEnvironmentId\": \"cbebxulwrbe6g7\",\n                \"DataZoneDomainRegion\": \"us-east-1\",\n            }\n        }\n    return _resource_metadata\nmetadata = _get_resource_metadata()",
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "\"\"\"\nLogging Configuration\n\nPurpose:\n--------\nThis sets up the logging framework for code executed in the user namespace.\n\"\"\"\n\nfrom typing import Optional\n\n\ndef _set_logging(log_dir: str, log_file: str, log_name: Optional[str] = None):\n    import os\n    import logging\n    from logging.handlers import RotatingFileHandler\n\n    level = logging.INFO\n    max_bytes = 5 * 1024 * 1024\n    backup_count = 5\n\n    # fallback to /tmp dir on access, helpful for local dev setup\n    try:\n        os.makedirs(log_dir, exist_ok=True)\n    except Exception:\n        log_dir = \"/tmp/kernels/\"\n\n    os.makedirs(log_dir, exist_ok=True)\n    log_path = os.path.join(log_dir, log_file)\n\n    logger = logging.getLogger() if not log_name else logging.getLogger(log_name)\n    logger.handlers = []\n    logger.setLevel(level)\n\n    formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n\n    # Rotating file handler\n    fh = RotatingFileHandler(filename=log_path, maxBytes=max_bytes, backupCount=backup_count, encoding=\"utf-8\")\n    fh.setFormatter(formatter)\n    logger.addHandler(fh)\n\n    logger.info(f\"Logging initialized for {log_name}.\")\n\n\n_set_logging(\"/var/log/computeEnvironments/kernel/\", \"kernel.log\")\n_set_logging(\"/var/log/studio/data-notebook-kernel-server/\", \"metrics.log\", \"metrics\")",
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import logging\nfrom sagemaker_studio import ClientConfig, sqlutils, sparkutils, dataframeutils\n\nlogger = logging.getLogger(__name__)\nlogger.info(\"Initializing sparkutils\")\nspark = sparkutils.init()\nlogger.info(\"Finished initializing sparkutils\")",
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def _reset_os_path():\n    \"\"\"\n    Reset the process's working directory to handle mount timing issues.\n    \n    This function resolves a race condition where the Python process starts\n    before the filesystem mount is complete, causing the process to reference\n    old mount paths and inodes. By explicitly changing to the mounted directory\n    (/home/sagemaker-user), we ensure the process uses the correct, up-to-date\n    mount point.\n    \n    The function logs stat information (device ID and inode) before and after\n    the directory change to verify that the working directory is properly\n    updated to reference the new mount.\n    \n    Note:\n        This is executed at module import time to ensure the fix is applied\n        as early as possible in the kernel initialization process.\n    \"\"\"\n    try:\n        import os\n        import logging\n\n        logger = logging.getLogger(__name__)\n        logger.info(\"---------Before------\")\n        logger.info(\"CWD: %s\", os.getcwd())\n        logger.info(\"stat('.'): %s %s\", os.stat('.').st_dev, os.stat('.').st_ino)\n        logger.info(\"stat('/home/sagemaker-user'): %s %s\", os.stat('/home/sagemaker-user').st_dev, os.stat('/home/sagemaker-user').st_ino)\n\n        os.chdir(\"/home/sagemaker-user\")\n\n        logger.info(\"---------After------\")\n        logger.info(\"CWD: %s\", os.getcwd())\n        logger.info(\"stat('.'): %s %s\", os.stat('.').st_dev, os.stat('.').st_ino)\n        logger.info(\"stat('/home/sagemaker-user'): %s %s\", os.stat('/home/sagemaker-user').st_dev, os.stat('/home/sagemaker-user').st_ino)\n    except Exception as e:\n        logger.exception(f\"Failed to reset working directory: {e}\")\n\n_reset_os_path()",
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Notebook"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Project 1: Demand Forecasting with LightGBM\n\n## 1. Introduction\nThis notebook demonstrates a complete pipeline for demand forecasting using LightGBM.\nWe will:\n- Load data from S3\n- Explore and preprocess the dataset\n- Create temporal features\n- Split the data into train and validation sets\n- Train a LightGBM model\n- Track parameters, metrics, and the model using MLflow"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Data Loading\nLoad the dataset from the S3 bucket."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import pandas as pd\r\nimport numpy as np\r\nimport sklearn\r\nimport mlflow\r\n\r\nprint(\"OK\")",
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "OK\n"
        },
        {
          "output_type": "display_data",
          "data": {},
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import boto3\r\nimport pandas as pd\r\nfrom io import BytesIO\r\n\r\nprint(\"STEP 1: creating S3 client\")\r\ns3 = boto3.client(\"s3\")\r\n\r\nBUCKET = \"ml-portfolio-av\"\r\nKEY = \"train.csv\"\r\n\r\nobj = s3.get_object(Bucket=BUCKET, Key=KEY)\r\ndf = pd.read_csv(BytesIO(obj[\"Body\"].read()))\r\n\r\nprint(\"DF LOADED\")\r\nprint(df.shape)",
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "STEP 1: creating S3 client\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "DF LOADED\n(3000888, 6)\n"
        },
        {
          "output_type": "display_data",
          "data": {},
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 3. Exploratory Data Analysis\n\nCheck columns, data types, and date range."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "print(df.columns)\r\nprint(df.dtypes)",
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Index(['id', 'date', 'store_nbr', 'family', 'sales', 'onpromotion'], dtype='object')\nid               int64\ndate            object\nstore_nbr        int64\nfamily          object\nsales          float64\nonpromotion      int64\ndtype: object\n"
        },
        {
          "output_type": "display_data",
          "data": {},
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "df[\"date\"] = pd.to_datetime(df[\"date\"])\r\nprint(df[\"date\"].min(), df[\"date\"].max())",
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {},
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2013-01-01 00:00:00 2017-08-15 00:00:00\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 4. Sorting and Aggregation\n\nSort data by date and aggregate sales by date and family."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "df = df.sort_values(\"date\").reset_index(drop=True)\r\nprint(\"OK - sorted\")",
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "OK - sorted\n"
        },
        {
          "output_type": "display_data",
          "data": {},
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "df_agg = (\r\n    df\r\n    .groupby([\"date\", \"family\"], as_index=False)\r\n    .agg({\"sales\": \"sum\"})\r\n)\r\n\r\nprint(df_agg.shape)\r\ndf_agg.head()\r\n",
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {},
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "(55572, 3)\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": null,
          "data": {
            "text/plain": "        date      family  sales\n0 2013-01-01  AUTOMOTIVE    0.0\n1 2013-01-01   BABY CARE    0.0\n2 2013-01-01      BEAUTY    2.0\n3 2013-01-01   BEVERAGES  810.0\n4 2013-01-01       BOOKS    0.0",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>family</th>\n      <th>sales</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2013-01-01</td>\n      <td>AUTOMOTIVE</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2013-01-01</td>\n      <td>BABY CARE</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2013-01-01</td>\n      <td>BEAUTY</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2013-01-01</td>\n      <td>BEVERAGES</td>\n      <td>810.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2013-01-01</td>\n      <td>BOOKS</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "application/vnd.datanotebooks+parquet+snappy": {
              "data": "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"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 5. Train / Validation Split\n\nSplit the dataset using a temporal split."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "split_date = \"2017-01-01\"\r\n\r\ntrain_df = df_agg[df_agg[\"date\"] < split_date]\r\nval_df   = df_agg[df_agg[\"date\"] >= split_date]\r\n\r\nprint(\"TRAIN:\", train_df.shape)\r\nprint(\"VAL:\", val_df.shape)\r\n",
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {},
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "TRAIN: (48081, 3)\nVAL: (7491, 3)\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 6. Feature Engineering\n\nAdd temporal features: day of week, month, day."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def add_time_features(df):\r\n    df = df.copy()\r\n    df[\"dayofweek\"] = df[\"date\"].dt.dayofweek\r\n    df[\"month\"] = df[\"date\"].dt.month\r\n    df[\"day\"] = df[\"date\"].dt.day\r\n    return df\r\n\r\ntrain_df = add_time_features(train_df)\r\nval_df   = add_time_features(val_df)\r\n\r\nprint(train_df.head())\r\n",
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {},
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "        date      family  sales  dayofweek  month  day\n0 2013-01-01  AUTOMOTIVE    0.0          1      1    1\n1 2013-01-01   BABY CARE    0.0          1      1    1\n2 2013-01-01      BEAUTY    2.0          1      1    1\n3 2013-01-01   BEVERAGES  810.0          1      1    1\n4 2013-01-01       BOOKS    0.0          1      1    1\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import lightgbm as lgb\r\nprint(\"LightGBM OK\", lgb.__version__)",
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "LightGBM OK 4.6.0\n"
        },
        {
          "output_type": "display_data",
          "data": {},
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Encode the categorical variable family."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "from sklearn.preprocessing import LabelEncoder\r\n\r\nle = LabelEncoder()\r\ntrain_df[\"family_enc\"] = le.fit_transform(train_df[\"family\"])\r\nval_df[\"family_enc\"] = le.transform(val_df[\"family\"])\r\n\r\nprint(train_df[[\"family\", \"family_enc\"]].head())",
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {},
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "       family  family_enc\n0  AUTOMOTIVE           0\n1   BABY CARE           1\n2      BEAUTY           2\n3   BEVERAGES           3\n4       BOOKS           4\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 7. Prepare Features and Target"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "FEATURES = [\"family_enc\", \"dayofweek\", \"month\", \"day\"]\r\nTARGET = \"sales\"\r\n\r\nX_train = train_df[FEATURES]\r\ny_train = train_df[TARGET]\r\n\r\nX_val = val_df[FEATURES]\r\ny_val = val_df[TARGET]\r\n\r\nprint(\"X_train shape:\", X_train.shape)\r\nprint(\"y_train shape:\", y_train.shape)",
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {},
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "X_train shape: (48081, 4)\ny_train shape: (48081,)\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 8. LightGBM Training\n\nTrain a LightGBM model on CPU."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import lightgbm as lgb\r\n\r\ntrain_data = lgb.Dataset(X_train, label=y_train)\r\nval_data = lgb.Dataset(X_val, label=y_val)\r\n\r\nparams = {\r\n    \"objective\": \"regression\",\r\n    \"metric\": \"rmse\",\r\n    \"verbose\": -1,\r\n    \"boosting_type\": \"gbdt\",\r\n    \"num_threads\": 2  # CPU ligero\r\n}\r\n\r\nprint(\"Training started...\")\r\n\r\ncallbacks = [lgb.early_stopping(stopping_rounds=10)]\r\n\r\nmodel = lgb.train(\r\n    params,\r\n    train_data,\r\n    valid_sets=[train_data, val_data],\r\n    num_boost_round=50,\r\n    callbacks=callbacks\r\n)\r\n\r\nprint(\"Training finished\")\r\n",
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {},
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Training started...\nTraining until validation scores don't improve for 10 rounds\nDid not meet early stopping. Best iteration is:\n[50]\ttraining's rmse: 14853.5\tvalid_1's rmse: 26160.3\nTraining finished\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 9. MLflow Tracking\n\nLog parameters, metrics, and model in MLflow."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import mlflow\r\nimport mlflow.lightgbm\r\nfrom sklearn.metrics import mean_squared_error\r\nimport numpy as np\r\n\r\nmlflow.set_experiment(\"Forecasting_Demand\")\r\n\r\nwith mlflow.start_run():\r\n    mlflow.log_params(params)\r\n    y_pred = model.predict(X_val)\r\n    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\r\n    mlflow.log_metric(\"rmse\", rmse)\r\n    mlflow.lightgbm.log_model(model, name=\"model\")\r\n\r\nprint(\"MLflow run logged, RMSE:\", rmse)\r\n\r\n",
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\u001b[31m2026/01/29 20:43:09 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
        },
        {
          "output_type": "display_data",
          "data": {},
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "MLflow run logged, RMSE: 26160.26321213675\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 10. Summary\n\n- Dataset loaded from S3 \n- Temporal features added\n- Train / Validation split applied\n- LightGBM trained on CPU\n- Metrics and model tracked in MLflow\n- Ready to deploy or integrate in production"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import os\r\nprint(os.path.expanduser(\"~\"))\r\n\r\n",
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "/home/sagemaker-user\n"
        },
        {
          "output_type": "display_data",
          "data": {},
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import os\r\n\r\n# Carpeta raíz del proyecto (local en tu home)\r\nproject_root = \"/home/sagemaker-user/project_forecasting_demand\"\r\n\r\n# Subcarpetas\r\nsubfolders = [\"notebooks\", \"scripts\", \"data\"]\r\n\r\n# Crear carpeta principal\r\nos.makedirs(project_root, exist_ok=True)\r\n\r\n# Crear subcarpetas\r\nfor f in subfolders:\r\n    os.makedirs(os.path.join(project_root, f), exist_ok=True)\r\n\r\nprint(\"Project folders created locally:\")\r\nprint(os.listdir(project_root))\r\n",
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {},
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Project folders created locally:\n['notebooks', 'scripts', 'data']\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import subprocess\r\n\r\n# Git configuration with your data\r\ngit_name = \"Javier-DataScience\"\r\ngit_email = \"alvaro.vega.vargas@gmail.com\"\r\n\r\nsubprocess.run([\"git\", \"config\", \"--global\", \"user.name\", git_name])\r\nsubprocess.run([\"git\", \"config\", \"--global\", \"user.email\", git_email])\r\n\r\nprint(\"Git configured successfully\")",
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {},
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Git configured successfully\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import shutil\r\nimport os\r\n\r\n# Nombre actual de tu notebook tal como aparece en SageMaker\r\ncurrent_notebook = \"/home/sagemaker-user/Forecasting Demand.ipynb\"\r\n\r\n# Carpeta destino\r\ndestination_folder = \"/home/sagemaker-user/project_forecasting_demand/notebooks/\"\r\n\r\n# Crear carpeta destino si no existe\r\nos.makedirs(destination_folder, exist_ok=True)\r\n\r\n# Nombre final del notebook\r\ndestination_notebook = os.path.join(destination_folder, \"forecasting_demand.ipynb\")\r\n\r\n# Mover el notebook\r\nshutil.move(current_notebook, destination_notebook)\r\n\r\nprint(\"Notebook moved to:\", destination_notebook)\r\n",
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/home/sagemaker-user/Forecasting Demand.ipynb'",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m/sagemaker_packages/python/cpython-3.11.14-linux-x86_64-gnu/lib/python3.11/shutil.py:853\u001b[39m, in \u001b[36mmove\u001b[39m\u001b[34m(src, dst, copy_function)\u001b[39m\n\u001b[32m    852\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m853\u001b[39m     \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_dst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    854\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/home/sagemaker-user/Forecasting Demand.ipynb' -> '/home/sagemaker-user/project_forecasting_demand/notebooks/forecasting_demand.ipynb'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m destination_notebook = os.path.join(destination_folder, \u001b[33m\"\u001b[39m\u001b[33mforecasting_demand.ipynb\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Mover el notebook\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mshutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmove\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_notebook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdestination_notebook\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNotebook moved to:\u001b[39m\u001b[33m\"\u001b[39m, destination_notebook)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/sagemaker_packages/python/cpython-3.11.14-linux-x86_64-gnu/lib/python3.11/shutil.py:873\u001b[39m, in \u001b[36mmove\u001b[39m\u001b[34m(src, dst, copy_function)\u001b[39m\n\u001b[32m    871\u001b[39m         rmtree(src)\n\u001b[32m    872\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         \u001b[43mcopy_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_dst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    874\u001b[39m         os.unlink(src)\n\u001b[32m    875\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m real_dst\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/sagemaker_packages/python/cpython-3.11.14-linux-x86_64-gnu/lib/python3.11/shutil.py:448\u001b[39m, in \u001b[36mcopy2\u001b[39m\u001b[34m(src, dst, follow_symlinks)\u001b[39m\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.path.isdir(dst):\n\u001b[32m    447\u001b[39m     dst = os.path.join(dst, os.path.basename(src))\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m \u001b[43mcopyfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    449\u001b[39m copystat(src, dst, follow_symlinks=follow_symlinks)\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dst\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/sagemaker_packages/python/cpython-3.11.14-linux-x86_64-gnu/lib/python3.11/shutil.py:256\u001b[39m, in \u001b[36mcopyfile\u001b[39m\u001b[34m(src, dst, follow_symlinks)\u001b[39m\n\u001b[32m    254\u001b[39m     os.symlink(os.readlink(src), dst)\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(src, \u001b[33m'\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fsrc:\n\u001b[32m    257\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    258\u001b[39m             \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(dst, \u001b[33m'\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fdst:\n\u001b[32m    259\u001b[39m                 \u001b[38;5;66;03m# macOS\u001b[39;00m\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/home/sagemaker-user/Forecasting Demand.ipynb'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import subprocess\r\nimport os\r\n\r\n# Carpeta raíz del proyecto\r\nproject_root = \"/home/sagemaker-user/project_forecasting_demand\"\r\n\r\n# Navegar a la carpeta del proyecto\r\nos.chdir(project_root)\r\n\r\n# Crear README.md con contenido básico\r\nreadme_content = \"\"\"\r\n# Project 1: Demand Forecasting with LightGBM\r\n\r\n## Overview\r\nThis project demonstrates a complete pipeline for demand forecasting using LightGBM.\r\nTracked with MLflow, fully reproducible in Amazon SageMaker.\r\n\r\n## Dataset\r\n- Source: Kaggle or internal CSV\r\n- Columns: id, date, store_nbr, family, sales, onpromotion\r\n- Stored in S3 bucket: ml-portfolio-av\r\n\r\n## Features\r\n- Temporal features: day of week, month, day\r\n- Categorical encoding for 'family'\r\n\r\n## Model\r\n- LightGBM regression\r\n- CPU training to save costs\r\n- Metrics: RMSE (Validation)\r\n\r\n## Folder Structure\r\nproject_forecasting_demand/\r\n├─ notebooks/\r\n├─ scripts/\r\n├─ data/\r\n├─ README.md\r\n\"\"\"\r\n\r\n# Guardar README.md\r\nwith open(\"README.md\", \"w\") as f:\r\n    f.write(readme_content)\r\n\r\nprint(\"README.md created\")\r\n\r\n# Inicializar git si no está inicializado\r\nsubprocess.run([\"git\", \"init\"])\r\n\r\n# Agregar archivos al staging\r\nsubprocess.run([\"git\", \"add\", \"notebooks/forecasting_demand.ipynb\", \"README.md\"])\r\n\r\n# Primer commit\r\nsubprocess.run([\"git\", \"commit\", \"-m\", \"Initial commit: Project 1 - Forecasting Demand\"])\r\n\r\nprint(\"First commit done\")\r\n",
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {},
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "README.md created\nInitialized empty Git repository in /home/sagemaker-user/project_forecasting_demand/.git/\nOn branch master\n\nInitial commit\n\nUntracked files:\n  (use \"git add <file>...\" to include in what will be committed)\n\tREADME.md\n\nnothing added to commit but untracked files present (use \"git add\" to track)\nFirst commit done\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "hint: Using 'master' as the name for the initial branch. This default branch name\nhint: is subject to change. To configure the initial branch name to use in all\nhint: of your new repositories, which will suppress this warning, call:\nhint: \nhint: \tgit config --global init.defaultBranch <name>\nhint: \nhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\nhint: 'development'. The just-created branch can be renamed via this command:\nhint: \nhint: \tgit branch -m <name>\nfatal: pathspec 'notebooks/forecasting_demand.ipynb' did not match any files\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Shutdown cells"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "\"\"\"\nStop spark session and associated Athena Spark session\n\"\"\"\n\nfrom IPython import get_ipython as _get_ipython\n_get_ipython().user_ns[\"spark\"].stop()",
      "execution_count": 0,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}